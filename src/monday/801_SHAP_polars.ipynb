{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy==1.24.4\n",
    "%pip install numba==0.57.1\n",
    "%pip install shap==0.46.0\n",
    "%pip install optuna==3.6.1\n",
    "%pip install scikit-learn==1.3.2\n",
    "%pip install seaborn==0.13.1\n",
    "%pip install python-dotenv\n",
    "%pip install polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5846,
     "status": "ok",
     "timestamp": 1729447103107,
     "user": {
      "displayName": "Alejandro Bolaños",
      "userId": "15513030842131101305"
     },
     "user_tz": 180
    },
    "id": "I6d2e7cMaRZ0",
    "outputId": "7817b67a-f9a5-4674-c15e-029bf29f5f23"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
    "\n",
    "from time import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35847,
     "status": "ok",
     "timestamp": 1729447138952,
     "user": {
      "displayName": "Alejandro Bolaños",
      "userId": "15513030842131101305"
     },
     "user_tz": 180
    },
    "id": "wo_mKHbH_GSE"
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Accedo a variables de entorno\n",
    "dataset_path = os.getenv('DATASET_PATH')\n",
    "dataset_file = os.getenv('DATASET_RAW_FILE')\n",
    "\n",
    "# Cargo el dataset\n",
    "dataset = pl.read_csv(dataset_path + dataset_file, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsimple = dataset.select(\n",
    "    [\n",
    "        \"numero_de_cliente\",\n",
    "        ((pl.col(\"foto_mes\") // 100) * 12 + pl.col(\"foto_mes\") % 100).alias(\"periodo0\"),\n",
    "        pl.arange(0, dataset.height, step=1).alias(\"pos\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dsimple = dsimple.sort(\"numero_de_cliente\", \"periodo0\")\n",
    "\n",
    "\n",
    "# calculo topes\n",
    "periodo_ultimo = dsimple.select(pl.max(\"periodo0\"))\n",
    "periodo_anteultimo = periodo_ultimo - 1\n",
    "\n",
    "\n",
    "dsimple = dsimple.select(\n",
    "    [\n",
    "        pl.all(),\n",
    "        pl.col(\"periodo0\").shift(-1).over(\"numero_de_cliente\").alias(\"periodo1\"),\n",
    "        pl.col(\"periodo0\").shift(-2).over(\"numero_de_cliente\").alias(\"periodo2\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# assign most common class values = \"CONTINUA\"\n",
    "dsimple = dsimple.with_columns(\n",
    "    pl.when(pl.col(\"periodo0\") < periodo_anteultimo)\n",
    "    .then(pl.lit(\"CONTINUA\"))\n",
    "    .alias(\"clase_ternaria\")\n",
    ")\n",
    "\n",
    "\n",
    "dsimple = dsimple.with_columns(\n",
    "    pl.when(\n",
    "        (pl.col(\"periodo0\") < periodo_ultimo)\n",
    "        & (\n",
    "            pl.col(\"periodo1\").is_null()\n",
    "            | ((pl.col(\"periodo0\") + 1) < pl.col(\"periodo1\"))\n",
    "        )\n",
    "    )\n",
    "    .then(pl.lit(\"BAJA+1\"))\n",
    "    .otherwise(pl.col(\"clase_ternaria\"))\n",
    "    .alias(\"clase_ternaria\")\n",
    ")\n",
    "\n",
    "\n",
    "dsimple = dsimple.with_columns(\n",
    "    pl.when(\n",
    "        (pl.col(\"periodo0\") < periodo_anteultimo)\n",
    "        & ((pl.col(\"periodo0\") + 1) == pl.col(\"periodo1\"))\n",
    "        & (\n",
    "            pl.col(\"periodo2\").is_null()\n",
    "            | ((pl.col(\"periodo0\") + 2) < pl.col(\"periodo2\"))\n",
    "        )\n",
    "    )\n",
    "    .then(pl.lit(\"BAJA+2\"))\n",
    "    .otherwise(pl.col(\"clase_ternaria\"))\n",
    "    .alias(\"clase_ternaria\")\n",
    ")\n",
    "\n",
    "\n",
    "dsimple.filter(pl.col(\"clase_ternaria\") == \"BAJA+1\").count()\n",
    "dsimple.filter(pl.col(\"clase_ternaria\") == \"BAJA+2\").count()\n",
    "\n",
    "\n",
    "# pego el resultado en el dataset original y grabo\n",
    "dsimple = dsimple.sort(\"pos\")\n",
    "dataset = dataset.with_columns([dsimple[\"clase_ternaria\"]])\n",
    "\n",
    "# Guardo dataset\n",
    "#dataset.write_csv(\"competencia_01_polars.csv\", separator=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of 'clase_ternaria'\n",
    "unique_values = dataset.get_column('clase_ternaria').unique()\n",
    "print(\"Unique values:\", unique_values)\n",
    "\n",
    "# Get value counts of 'clase_ternaria'\n",
    "#category_counts = dataset.groupby(\"clase_ternaria\").count()\n",
    "#category_counts = dataset.groupby(\"clase_ternaria\").agg(pl.count().alias(\"count\"))\n",
    "category_counts = dataset.get_column('clase_ternaria').value_counts()\n",
    "print(\"Category counts:\")\n",
    "print(category_counts)\n",
    "\n",
    "# Preview the data (first few rows)\n",
    "print(\"Data preview:\")\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data['mpayroll_sobre_edad'] = data['mpayroll'] / data['cliente_edad']\n",
    "\n",
    "# Variables de sumas\n",
    "data['vm_mfinanciacion_limite'] = data[['Master_mfinanciacion_limite', 'Visa_mfinanciacion_limite']].sum(axis=1, skipna=True)\n",
    "data['vm_Fvencimiento'] = data[['Master_Fvencimiento', 'Visa_Fvencimiento']].min(axis=1, skipna=True)\n",
    "data['vm_Finiciomora'] = data[['Master_Finiciomora', 'Visa_Finiciomora']].min(axis=1, skipna=True)\n",
    "data['vm_msaldototal'] = data[['Master_msaldototal', 'Visa_msaldototal']].sum(axis=1, skipna=True)\n",
    "data['vm_msaldopesos'] = data[['Master_msaldopesos', 'Visa_msaldopesos']].sum(axis=1, skipna=True)\n",
    "data['vm_msaldodolares'] = data[['Master_msaldodolares', 'Visa_msaldodolares']].sum(axis=1, skipna=True)\n",
    "data['vm_mconsumospesos'] = data[['Master_mconsumospesos', 'Visa_mconsumospesos']].sum(axis=1, skipna=True)\n",
    "data['vm_mconsumosdolares'] = data[['Master_mconsumosdolares', 'Visa_mconsumosdolares']].sum(axis=1, skipna=True)\n",
    "data['vm_mlimitecompra'] = data[['Master_mlimitecompra', 'Visa_mlimitecompra']].sum(axis=1, skipna=True)\n",
    "data['vm_madelantopesos'] = data[['Master_madelantopesos', 'Visa_madelantopesos']].sum(axis=1, skipna=True)\n",
    "data['vm_madelantodolares'] = data[['Master_madelantodolares', 'Visa_madelantodolares']].sum(axis=1, skipna=True)\n",
    "data['vm_fultimo_cierre'] = data[['Master_fultimo_cierre', 'Visa_fultimo_cierre']].max(axis=1, skipna=True)\n",
    "data['vm_mpagado'] = data[['Master_mpagado', 'Visa_mpagado']].sum(axis=1, skipna=True)\n",
    "data['vm_mpagospesos'] = data[['Master_mpagospesos', 'Visa_mpagospesos']].sum(axis=1, skipna=True)\n",
    "data['vm_mpagosdolares'] = data[['Master_mpagosdolares', 'Visa_mpagosdolares']].sum(axis=1, skipna=True)\n",
    "data['vm_fechaalta'] = data[['Master_fechaalta', 'Visa_fechaalta']].max(axis=1, skipna=True)\n",
    "data['vm_mconsumototal'] = data[['Master_mconsumototal', 'Visa_mconsumototal']].sum(axis=1, skipna=True)\n",
    "data['vm_cconsumos'] = data[['Master_cconsumos', 'Visa_cconsumos']].sum(axis=1, skipna=True)\n",
    "data['vm_cadelantosefectivo'] = data[['Master_cadelantosefectivo', 'Visa_cadelantosefectivo']].sum(axis=1, skipna=True)\n",
    "data['vm_mpagominimo'] = data[['Master_mpagominimo', 'Visa_mpagominimo']].sum(axis=1, skipna=True)\n",
    "\n",
    "# Variables de ratios\n",
    "data['vmr_Master_mlimitecompra'] = data['Master_mlimitecompra'] / data['vm_mlimitecompra']\n",
    "data['vmr_Visa_mlimitecompra'] = data['Visa_mlimitecompra'] / data['vm_mlimitecompra']\n",
    "data['vmr_msaldototal'] = data['vm_msaldototal'] / data['vm_mlimitecompra']\n",
    "data['vmr_msaldopesos'] = data['vm_msaldopesos'] / data['vm_mlimitecompra']\n",
    "data['vmr_msaldopesos2'] = data['vm_msaldopesos'] / data['vm_msaldototal']\n",
    "data['vmr_msaldodolares'] = data['vm_msaldodolares'] / data['vm_mlimitecompra']\n",
    "data['vmr_msaldodolares2'] = data['vm_msaldodolares'] / data['vm_msaldototal']\n",
    "data['vmr_mconsumospesos'] = data['vm_mconsumospesos'] / data['vm_mlimitecompra']\n",
    "data['vmr_mconsumosdolares'] = data['vm_mconsumosdolares'] / data['vm_mlimitecompra']\n",
    "data['vmr_madelantopesos'] = data['vm_madelantopesos'] / data['vm_mlimitecompra']\n",
    "data['vmr_madelantodolares'] = data['vm_madelantodolares'] / data['vm_mlimitecompra']\n",
    "data['vmr_mpagado'] = data['vm_mpagado'] / data['vm_mlimitecompra']\n",
    "data['vmr_mpagospesos'] = data['vm_mpagospesos'] / data['vm_mlimitecompra']\n",
    "data['vmr_mpagosdolares'] = data['vm_mpagosdolares'] / data['vm_mlimitecompra']\n",
    "data['vmr_mconsumototal'] = data['vm_mconsumototal'] / data['vm_mlimitecompra']\n",
    "data['vmr_mpagominimo'] = data['vm_mpagominimo'] / data['vm_mlimitecompra']\n",
    "\n",
    "\n",
    "\n",
    "# Filtramos solo las columnas numéricas\n",
    "numeric_cols = data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Reemplazo valores infinitos con NaN solo en las columnas numéricas\n",
    "infinitos_qty = np.isinf(numeric_cols).sum().sum()\n",
    "if infinitos_qty > 0:\n",
    "    print(f\"ATENCIÓN: Hay {infinitos_qty} valores infinitos en tu dataset. Serán pasados a NaN.\")\n",
    "    data[numeric_cols.columns] = numeric_cols.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Valvula de seguridad para evitar valores NaN\n",
    "nans_qty = data.isna().sum().sum()\n",
    "if nans_qty > 0:\n",
    "    print(f\"ATENCIÓN: Hay {nans_qty} valores NaN en tu dataset. Serán pasados arbitrariamente a 0.\")\n",
    "    # Reemplazo valores NaN con 0\n",
    "    data.fillna(0, inplace=True)\n",
    "\n",
    "data.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Definimos las columnas que NO vamos a utilizar para crear los lags\n",
    "campitos = [\"numero_de_cliente\", \"foto_mes\", \"clase_ternaria\"]\n",
    "\n",
    "# Seleccionamos todas las columnas lagueables (que no están en campitos)\n",
    "cols_lagueables = [col for col in data.columns if col not in campitos]  \n",
    "\n",
    "# Ordenamos el DataFrame por 'numero_de_cliente' y 'foto_mes'\n",
    "data.sort_values(by=[\"numero_de_cliente\", \"foto_mes\"], inplace=True)\n",
    "\n",
    "# Creamos los lags de orden 1 para las columnas en cols_lagueables\n",
    "#for col in cols_lagueables:\n",
    "    # Creamos la columna lag1 (desplazamiento hacia abajo de 1 periodo)\n",
    "#    data[f\"{col}_lag1\"] = data.groupby(\"numero_de_cliente\")[col].shift(1)\n",
    "\n",
    "# Creamos los delta lags de orden 1\n",
    "#for col in cols_lagueables:\n",
    "    # Calculamos la diferencia entre el valor actual y el valor lag1\n",
    "#    data[f\"{col}_delta1\"] = data[col] - data[f\"{col}_lag1\"]\n",
    "\n",
    "# Creamos un array vacío para los nuevos datos (lags y deltas)\n",
    "lags = {}\n",
    "deltas = {}\n",
    "\n",
    "# Creamos los lags y deltas usando NumPy para evitar la fragmentación\n",
    "for col in cols_lagueables:\n",
    "    # Obtenemos el grupo por 'numero_de_cliente'\n",
    "    grouped = data.groupby(\"numero_de_cliente\")[col]\n",
    "    \n",
    "    # Calculamos el lag usando shift con NumPy\n",
    "    lags[f\"{col}_lag1\"] = grouped.shift(1).to_numpy()\n",
    "    \n",
    "    # Calculamos el delta como la diferencia actual - lag1\n",
    "    deltas[f\"{col}_delta1\"] = data[col].to_numpy() - lags[f\"{col}_lag1\"]\n",
    "\n",
    "# Asignamos los resultados de lag y delta directamente al DataFrame\n",
    "for col_lag, values_lag in lags.items():\n",
    "    data[col_lag] = values_lag\n",
    "    \n",
    "for col_delta, values_delta in deltas.items():\n",
    "    data[col_delta] = values_delta\n",
    "\n",
    "\n",
    "data.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6389,
     "status": "ok",
     "timestamp": 1729447145807,
     "user": {
      "displayName": "Alejandro Bolaños",
      "userId": "15513030842131101305"
     },
     "user_tz": 180
    },
    "id": "gJFi741zaRZ3"
   },
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "Xtrain = dataset.filter(pl.col('foto_mes').is_in([202104, 202105, 202106]))\n",
    "\n",
    "# Create target variable - in polars we use map_elements for lambda functions\n",
    "ytrain = (Xtrain\n",
    "         .get_column('clase_ternaria')\n",
    "         .map_elements(lambda x: 0 if x == \"CONTINUA\" else 1, return_dtype=pl.Int32)\n",
    "         .to_numpy())  # Convert to numpy for LightGBM\n",
    "\n",
    "# Drop the target column\n",
    "Xtrain = Xtrain.drop('clase_ternaria')\n",
    "\n",
    "# Convert to numpy for LightGBM, preserving column names\n",
    "Xtrain_numpy = Xtrain.to_numpy()\n",
    "\n",
    "# Create LightGBM dataset\n",
    "lgb_train = lgb.Dataset(Xtrain_numpy, ytrain, feature_name=Xtrain.columns)\n",
    "\n",
    "# Parameters remain the same\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 2,\n",
    "    'max_bin': 15,\n",
    "    'min_data_in_leaf': 500,\n",
    "    'verbose': 0,\n",
    "}\n",
    "\n",
    "# Train model\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100)\n",
    "\n",
    "# Create importance DataFrame - converting back to pandas for compatibility\n",
    "lgbm_importancia = pl.DataFrame({\n",
    "    'Features': gbm.feature_name(),\n",
    "    'Importances': gbm.feature_importance()\n",
    "}).sort('Importances', descending=True)\n",
    "\n",
    "# If you need it as pandas DataFrame:\n",
    "# lgbm_importancia = lgbm_importancia.to_pandas()\n",
    "\n",
    "lgbm_importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1729448313925,
     "user": {
      "displayName": "Alejandro Bolaños",
      "userId": "15513030842131101305"
     },
     "user_tz": 180
    },
    "id": "sv4KdkYvIh_E",
    "outputId": "c4b19cdc-d3da-43da-fad5-6521392887c6"
   },
   "outputs": [],
   "source": [
    "# Calculo la importancia de las variables usando SHAP\n",
    "\n",
    "explainer = shap.TreeExplainer(gbm)\n",
    "shap_values = explainer.shap_values(Xtrain_numpy)\n",
    "\n",
    "shap_df = pl.DataFrame(shap_values, schema=Xtrain.columns)\n",
    "\n",
    "shap_importancia = pl.DataFrame({\n",
    "    'Feature': Xtrain.columns,\n",
    "    'SHAP Importance': np.abs(shap_values).mean(0)\n",
    "}).sort('SHAP Importance', descending=True)\n",
    "\n",
    "shap_importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 841
    },
    "executionInfo": {
     "elapsed": 32677,
     "status": "ok",
     "timestamp": 1729447613650,
     "user": {
      "displayName": "Alejandro Bolaños",
      "userId": "15513030842131101305"
     },
     "user_tz": 180
    },
    "id": "SEF-xuD8aRZ5",
    "outputId": "f937015b-9bf1-4e87-d717-5bcfd028e17e"
   },
   "outputs": [],
   "source": [
    "# Visualizo\n",
    "\n",
    "shap.summary_plot(shap_values, Xtrain_numpy, feature_names=Xtrain.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.drop(['cprestamos_personales', 'mprestamos_personales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprime primeras 5 filas del df\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar la ruta completa del archivo donde deseas guardar el DataFrame\n",
    "output_file = dataset_path + \"competencia_02_polar.csv\"\n",
    "\n",
    "# Guardar el DataFrame como un archivo CSV en la ruta especificada\n",
    "dataset.write_csv(output_file)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
